{
    "experiment_name": "multitask_bert_large_240kdata",
    "log_dirctory": "coda21_evaluation",
    "dataset_params": {
        "dataset_name": "coda21",
        "hf_tokenizer_name": "bert-large-uncased",
        "hf_tokenizer_path": "/scratch/c.scmag3/hf_pretrained_models/bert_large_mlm/tokenizer",
        "max_len": 120
    },
    "model_params": {
        "hf_checkpoint_name": "bert-large-uncased",
        "hf_model_path": "/scratch/c.scmag3/hf_pretrained_models/bert_large_mlm/model",
        "run_mode": "inference"
    },
    "inference_params": {
        "model_type": "multitask",
    	"data_folder": "data/coda21",
    	"sentence_model": "multitask_bert_large_240kdata",
    	"save_dir": "output",
        "pretrained_mention_model_path": null,
        "pretrained_definition_model_path": null,
        "pretrained_multitask_model_path": "/scratch/c.scmag3/encoder_mentions/trained_models/multitask_encoder_pretrain/bert_large_multitask_240kdata_pretrain.pt"

    }
}
